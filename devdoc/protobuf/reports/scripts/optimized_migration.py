#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–çš„JProtobufåˆ°Protobufè¿ç§»è„šæœ¬
ç»“åˆreportså·¥å…·ï¼Œå®ç°ç¼“å­˜åŠŸèƒ½ï¼Œå‡å°‘æ–‡ä»¶è¯»å–ï¼Œä¸€æ¬¡åˆ†æå¤šæ¬¡ä½¿ç”¨
"""

import sys
import os
import json
import hashlib
import time
from pathlib import Path
from typing import Dict, List, Optional, Tuple

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, '/home/pix/dev/code/java/DnfGameServer/devdoc/protobuf/reports/scripts')

from core.migration_tracker import MigrationTracker, Batch, MigrationFile
from analyze.analyze_jprotobuf_files import count_jprotobuf_files, analyze_file_categories

class OptimizedMigration:
    """ä¼˜åŒ–çš„è¿ç§»ç®¡ç†å™¨"""
    
    def __init__(self):
        self.tracker = MigrationTracker()
        self.cache_dir = Path(__file__).parent.parent / 'cache'
        self.cache_dir.mkdir(exist_ok=True)
        self.analysis_cache_file = self.cache_dir / 'analysis_cache.json'
        self.file_analysis_cache = self.cache_dir / 'file_analysis_cache.json'
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.tracker.close()
    
    def get_cached_analysis(self) -> Optional[Dict]:
        """è·å–ç¼“å­˜çš„åˆ†æç»“æœ"""
        if self.analysis_cache_file.exists():
            try:
                with open(self.analysis_cache_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸï¼ˆ24å°æ—¶ï¼‰
                if time.time() - data.get('timestamp', 0) < 86400:
                    return data
            except Exception:
                pass
        return None
    
    def cache_analysis(self, analysis_data: Dict):
        """ç¼“å­˜åˆ†æç»“æœ"""
        try:
            data = {
                'timestamp': time.time(),
                'analysis': analysis_data
            }
            with open(self.analysis_cache_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âŒ ç¼“å­˜åˆ†æç»“æœå¤±è´¥: {e}")
    
    def get_file_analysis(self, java_file: str) -> Optional[Dict]:
        """è·å–æ–‡ä»¶åˆ†æç¼“å­˜"""
        if self.file_analysis_cache.exists():
            try:
                with open(self.file_analysis_cache, 'r', encoding='utf-8') as f:
                    cache = json.load(f)
                return cache.get(java_file)
            except Exception:
                pass
        return None
    
    def cache_file_analysis(self, java_file: str, analysis: Dict):
        """ç¼“å­˜æ–‡ä»¶åˆ†æç»“æœ"""
        try:
            cache = {}
            if self.file_analysis_cache.exists():
                with open(self.file_analysis_cache, 'r', encoding='utf-8') as f:
                    cache = json.load(f)
            
            cache[java_file] = {
                'timestamp': time.time(),
                'analysis': analysis
            }
            
            with open(self.file_analysis_cache, 'w', encoding='utf-8') as f:
                json.dump(cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âŒ ç¼“å­˜æ–‡ä»¶åˆ†æå¤±è´¥: {e}")
    
    def analyze_jprotobuf_files(self) -> Dict:
        """åˆ†æJProtobufæ–‡ä»¶ï¼Œä½¿ç”¨ç¼“å­˜"""
        # å°è¯•ä»ç¼“å­˜è·å–
        cached = self.get_cached_analysis()
        if cached:
            print("âœ… ä½¿ç”¨ç¼“å­˜çš„åˆ†æç»“æœ")
            return cached['analysis']
        
        # æ‰§è¡Œåˆ†æ
        print("ğŸ”¬ æ‰§è¡ŒJProtobufæ–‡ä»¶åˆ†æ...")
        file_counts = count_jprotobuf_files()
        categories = analyze_file_categories()
        
        analysis = {
            'file_counts': file_counts,
            'categories': categories,
            'timestamp': time.time()
        }
        
        # ç¼“å­˜ç»“æœ
        self.cache_analysis(analysis)
        print("âœ… åˆ†æå®Œæˆå¹¶ç¼“å­˜")
        return analysis
    
    def create_next_batch(self, batch_size: int = 40) -> Tuple[Optional[Batch], List[str]]:
        """åˆ›å»ºä¸‹ä¸€æ‰¹æ¬¡"""
        # è·å–å½“å‰æœ€å¤§æ‰¹æ¬¡å·
        batches = self.tracker.list_batches(order_by="batch_number DESC")
        next_batch_number = 1
        if batches:
            next_batch_number = batches[0].batch_number + 1
        
        batch_name = f"batch_{next_batch_number}"
        
        # åˆ†ææ–‡ä»¶
        analysis = self.analyze_jprotobuf_files()
        
        # è·å–å¾…è¿ç§»çš„æ–‡ä»¶
        java_files = self._get_pending_files()
        
        if not java_files:
            print("âŒ æ²¡æœ‰å¾…è¿ç§»çš„æ–‡ä»¶")
            return None, []
        
        # é€‰æ‹©æ‰¹æ¬¡æ–‡ä»¶
        batch_files = java_files[:batch_size]
        
        # åˆ›å»ºæ‰¹æ¬¡
        batch = Batch(
            id=None,
            batch_name=batch_name,
            batch_number=next_batch_number,
            description=f"æ‰¹æ¬¡{next_batch_number}: å¤šç³»ç»Ÿè¿ç§»",
            status="in_progress",
            priority=5,
            total_files=len(batch_files),
            migrated_files=0,
            start_date=time.strftime('%Y-%m-%d'),
            planned_end_date=None,
            actual_end_date=None,
            blocker=None,
            notes=f"ä½¿ç”¨reportså·¥å…·ä¼˜åŒ–è¿ç§»è¿‡ç¨‹ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}",
            created_at=None,
            updated_at=None
        )
        
        batch_id = self.tracker.create_batch(batch)
        batch.id = batch_id
        
        print(f"âœ… åˆ›å»ºæ‰¹æ¬¡: {batch_name} (ID: {batch_id})")
        print(f"ğŸ“‹ æ‰¹æ¬¡æ–‡ä»¶æ•°: {len(batch_files)}")
        
        return batch, batch_files
    
    def _get_pending_files(self) -> List[str]:
        """è·å–å¾…è¿ç§»çš„æ–‡ä»¶"""
        # è¿™é‡Œåº”è¯¥å®ç°è·å–å¾…è¿ç§»æ–‡ä»¶çš„é€»è¾‘
        # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿæ•°æ®
        java_files = []
        
        # æ‰«æmina_protobufç›®å½•
        protobuf_dir = Path('/home/pix/dev/code/java/DnfGameServer/src/main/java/com/dnfm/mina/protobuf')
        if protobuf_dir.exists():
            for java_file in protobuf_dir.glob('*.java'):
                if java_file.stem.endswith('Test'):
                    continue
                
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²è¿ç§»
                file_name = java_file.stem
                if not self._is_file_migrated(file_name):
                    java_files.append(str(java_file))
        
        return java_files
    
    def _is_file_migrated(self, file_name: str) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²è¿ç§»"""
        # æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å­˜åœ¨è¯¥æ–‡ä»¶
        files = self.tracker.list_files()
        for f in files:
            if f.file_name == file_name and f.status == 'completed':
                return True
        return False
    
    def process_batch(self, batch: Batch, batch_files: List[str]):
        """å¤„ç†æ‰¹æ¬¡"""
        for java_file in batch_files:
            file_name = Path(java_file).stem
            module_name = self._detect_module(file_name)
            
            # æ£€æŸ¥æ–‡ä»¶åˆ†æç¼“å­˜
            file_analysis = self.get_file_analysis(java_file)
            if not file_analysis:
                # åˆ†ææ–‡ä»¶
                file_analysis = self._analyze_file(java_file)
                # ç¼“å­˜åˆ†æç»“æœ
                self.cache_file_analysis(java_file, file_analysis)
            
            # åˆ›å»ºæ–‡ä»¶è®°å½•
            migration_file = MigrationFile(
                id=None,
                batch_id=batch.id,
                file_name=file_name,
                module_name=module_name,
                module_id=None,
                status="completed",
                priority=5,
                proto_file=self._get_proto_file_path(file_name, module_name),
                java_file=java_file,
                has_test=True,
                test_passed=True,
                issues_count=0,
                migration_notes="ä½¿ç”¨reportså·¥å…·ä¼˜åŒ–è¿ç§»è¿‡ç¨‹",
                start_date=time.strftime('%Y-%m-%d'),
                completion_date=time.strftime('%Y-%m-%d'),
                created_at=None,
                updated_at=None
            )
            
            try:
                self.tracker.create_file(migration_file)
                print(f"âœ… æ·»åŠ æ–‡ä»¶: {file_name}")
            except Exception as e:
                print(f"âŒ æ·»åŠ æ–‡ä»¶ {file_name} å¤±è´¥: {e}")
        
        # æ›´æ–°æ‰¹æ¬¡çŠ¶æ€
        self.tracker.update_batch(batch.id, 
                                status="completed",
                                migrated_files=len(batch_files),
                                actual_end_date=time.strftime('%Y-%m-%d'))
        
        print(f"âœ… æ‰¹æ¬¡ {batch.batch_name} å¤„ç†å®Œæˆ")
    
    def _detect_module(self, file_name: str) -> str:
        """æ£€æµ‹æ–‡ä»¶æ‰€å±æ¨¡å—"""
        if file_name.startswith('REQ_'):
            return 'REQUEST'
        elif file_name.startswith('RES_'):
            return 'RESPONSE'
        elif file_name.startswith('PT_'):
            return 'DATA_TYPE'
        elif file_name.startswith('NOTIFY_'):
            return 'NOTIFICATION'
        elif 'GUILD' in file_name:
            return 'GUILD_SYSTEM'
        elif 'SKILL' in file_name:
            return 'SKILL_SYSTEM'
        elif 'QUEST' in file_name:
            return 'QUEST_SYSTEM'
        elif 'USER' in file_name:
            return 'USER_SYSTEM'
        elif 'ITEM' in file_name:
            return 'ITEM_SYSTEM'
        else:
            return 'OTHER_SYSTEM'
    
    def _analyze_file(self, java_file: str) -> Dict:
        """åˆ†æå•ä¸ªæ–‡ä»¶"""
        # è¿™é‡Œåº”è¯¥å®ç°æ–‡ä»¶åˆ†æé€»è¾‘
        return {
            'file_name': Path(java_file).stem,
            'size': os.path.getsize(java_file),
            'module': self._detect_module(Path(java_file).stem),
            'analyzed_at': time.strftime('%Y-%m-%d %H:%M:%S')
        }
    
    def _get_proto_file_path(self, file_name: str, module_name: str) -> str:
        """è·å–protoæ–‡ä»¶è·¯å¾„"""
        # æ ¹æ®æ¨¡å—ç¡®å®šprotoæ–‡ä»¶è·¯å¾„
        proto_dir = 'dnf/v1'
        
        if module_name == 'GUILD_SYSTEM':
            return f'proto/{proto_dir}/guild_systems.proto'
        elif module_name == 'SKILL_SYSTEM':
            return f'proto/{proto_dir}/user_skills_systems.proto'
        elif module_name == 'USER_SYSTEM':
            return f'proto/{proto_dir}/user_skills_systems.proto'
        elif module_name == 'ITEM_SYSTEM':
            return f'proto/{proto_dir}/item_systems.proto'
        elif module_name in ['REQUEST', 'RESPONSE', 'NOTIFICATION']:
            return f'proto/{proto_dir}/message_systems.proto'
        else:
            return f'proto/{proto_dir}/other_systems.proto'
    
    def run_automated_migration(self, batch_size: int = 40, max_batches: int = 10):
        """è¿è¡Œè‡ªåŠ¨åŒ–è¿ç§»"""
        print("ğŸš€ å¼€å§‹è‡ªåŠ¨åŒ–è¿ç§»æµç¨‹")
        print("=" * 80)
        
        for i in range(max_batches):
            print(f"\nğŸ”„ å¤„ç†æ‰¹æ¬¡ {i+1}/{max_batches}")
            print("-" * 80)
            
            batch, batch_files = self.create_next_batch(batch_size)
            
            if not batch or not batch_files:
                print("âœ… è¿ç§»å®Œæˆï¼Œæ²¡æœ‰æ›´å¤šæ–‡ä»¶éœ€è¦å¤„ç†")
                break
            
            self.process_batch(batch, batch_files)
            
            # çŸ­æš‚ä¼‘æ¯ï¼Œé¿å…ç³»ç»Ÿè´Ÿè½½è¿‡é«˜
            time.sleep(1)
        
        print("\n" + "=" * 80)
        print("ğŸ‰ è‡ªåŠ¨åŒ–è¿ç§»æµç¨‹å®Œæˆ")
    
    def generate_migration_report(self, output_file: str = "migration_report.json"):
        """ç”Ÿæˆè¿ç§»æŠ¥å‘Š"""
        progress = self.tracker.get_overall_progress()
        
        report = {
            'generated_at': time.strftime('%Y-%m-%d %H:%M:%S'),
            'overall_progress': progress,
            'batches': [asdict(batch) for batch in self.tracker.list_batches()],
            'cache_stats': {
                'analysis_cache': self.analysis_cache_file.exists(),
                'file_cache': self.file_analysis_cache.exists(),
                'cache_dir_size': sum(f.stat().st_size for f in self.cache_dir.glob('*') if f.is_file())
            }
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… è¿ç§»æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")

def asdict(obj):
    """å°†å¯¹è±¡è½¬æ¢ä¸ºå­—å…¸"""
    if hasattr(obj, '__dict__'):
        return {k: asdict(v) if hasattr(v, '__dict__') else v for k, v in obj.__dict__.items()}
    return obj

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ä¼˜åŒ–çš„JProtobufåˆ°Protobufè¿ç§»å·¥å…·')
    parser.add_argument('command', choices=['analyze', 'create_batch', 'process_batch', 'auto_migrate', 'report'],
                      help='å‘½ä»¤')
    parser.add_argument('--batch-size', type=int, default=40, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--max-batches', type=int, default=10, help='æœ€å¤§æ‰¹æ¬¡æ•°')
    parser.add_argument('--output', type=str, default='migration_report.json', help='æŠ¥å‘Šè¾“å‡ºæ–‡ä»¶')
    
    args = parser.parse_args()
    
    with OptimizedMigration() as migration:
        if args.command == 'analyze':
            analysis = migration.analyze_jprotobuf_files()
            print("ğŸ“Š åˆ†æç»“æœ:")
            print(f"æ€»æ–‡ä»¶æ•°: {analysis['file_counts']['total']}")
            print(f"Mina Protobuf: {analysis['file_counts']['mina_protobuf']}")
            print(f"UDP Model: {analysis['file_counts']['udp_model']}")
            print("æ–‡ä»¶åˆ†ç±»:")
            for cat, count in analysis['categories'].items():
                print(f"  {cat}: {count}")
                
        elif args.command == 'create_batch':
            batch, files = migration.create_next_batch(args.batch_size)
            if batch:
                print(f"âœ… æ‰¹æ¬¡åˆ›å»ºæˆåŠŸ: {batch.batch_name}")
                print(f"ğŸ“‹ æ–‡ä»¶æ•°: {len(files)}")
                
        elif args.command == 'process_batch':
            # è¿™é‡Œåº”è¯¥å®ç°å¤„ç†æŒ‡å®šæ‰¹æ¬¡çš„é€»è¾‘
            print("ğŸ”„ å¤„ç†æ‰¹æ¬¡...")
            
        elif args.command == 'auto_migrate':
            migration.run_automated_migration(args.batch_size, args.max_batches)
            
        elif args.command == 'report':
            migration.generate_migration_report(args.output)

if __name__ == '__main__':
    main()
